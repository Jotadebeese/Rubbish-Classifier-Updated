{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZ9Xe6KFT94Cexi+ga9+pP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jotadebeese/Rubbish-Classifier-Updated/blob/main/paper_replicating/paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper Replicating\n",
        "\n",
        "Turn a ML research paper into usable code. The [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) paper using PyTorch."
      ],
      "metadata": {
        "id": "l2UaqZtr37FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "WH46qGTaByHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHJmbodBCM-h",
        "outputId": "41d2eeda-4f6d-4ff7-e596-1affbe8ce4d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "0.16.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the PyTorch Scripts directory, download it from GitHub\n",
        "try:\n",
        "    from modular_scripts import data_setup, engine, utils\n",
        "except:\n",
        "    # Get the scripts\n",
        "    print(\"[INFO] Couldn't find scripts, downloading them from GitHub.\")\n",
        "    !git clone https://github.com/Jotadebeese/pytorch_scripts\n",
        "    !mv pytorch_scripts/modular_scripts .\n",
        "    !rm -rf pytorch_scripts\n",
        "    from modular_scripts import data_setup, engine, utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGZZ4AINCUS5",
        "outputId": "0c8d70eb-ff41-4b98-81d2-0ea7e104e652"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find scripts, downloading them from GitHub.\n",
            "Cloning into 'pytorch_scripts'...\n",
            "remote: Enumerating objects: 227, done.\u001b[K\n",
            "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 227 (delta 127), reused 201 (delta 102), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (227/227), 54.33 KiB | 912.00 KiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "[INFO] splitfolders module not found. Installing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_dFki_LFLEr",
        "outputId": "5e093002-cc1b-4684-c374-b7b480dff5a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data and Data Preparation"
      ],
      "metadata": {
        "id": "MKCQmAlxF8Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modular_scripts.utils import download_data\n",
        "\n",
        "# Download the data\n",
        "image_path = download_data(source='1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8',\n",
        "                           destination='augmented_rubbish_dataset',\n",
        "                           from_gdrive=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "xs9irqThFzHb",
        "outputId": "df217f42-7110-42fc-9606-5f69e8e5a3f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data/augmented_rubbish_dataset' directory already exists, skipping directory creation...\n",
            "[INFO] Donwloading 1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8 from https://drive.google.com/uc?id=1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8\n",
            "Access denied with the following error:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8 \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-63f1a6f7e25d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Download the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m image_path = download_data(source='1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8',\n\u001b[0m\u001b[1;32m      5\u001b[0m                            \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'augmented_rubbish_dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            from_gdrive=True)\n",
            "\u001b[0;32m/content/modular_scripts/utils.py\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m(source, destination, from_gdrive, remove_source)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Unzip data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Unzipping {target_file} data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/1jGCybRl5NkbGM6EnrK8wri5FeqJrSXg8'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.bulk_image_convertor(dataset_path=str(image_path) + '/rubbish_dataset_augmented',\n",
        "                    format=\"jpg\")"
      ],
      "metadata": {
        "id": "FSNywOzuG3Gb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "6d91afac-198d-4737-806f-ca188a7741b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d823cbb6bc19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m utils.bulk_image_convertor(dataset_path=str(image_path) + '/rubbish_dataset_augmented',\n\u001b[0m\u001b[1;32m      2\u001b[0m                     format=\"jpg\")\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir, test_dir, val_dir = utils.split_data(\n",
        "    input_folder=str(image_path) + '/rubbish_dataset_augmented',\n",
        "    output_folder=image_path\n",
        ")"
      ],
      "metadata": {
        "id": "5oOyH5jnIHMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir, test_dir, val_dir"
      ],
      "metadata": {
        "id": "3ZFQRLG3JEIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create DataLoaders"
      ],
      "metadata": {
        "id": "ZA43O5I3JUXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modular_scripts.data_setup import create_dataLoaders\n",
        "from torchvision import transforms\n",
        "\n",
        "# Create image size\n",
        "IMG_SIZE = 224 # from ViT research paper\n",
        "\n",
        "# Create transforms pipeline\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataLoader, test_dataLoader, class_names = create_dataLoaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "print(f\"Number of train images: {len(train_dataLoader.dataset)}, Train DataLoaders: {len(train_dataLoader)}\")\n",
        "print(f\"Number of test images: {len(test_dataLoader.dataset)}, Test DataLoaders: {len(test_dataLoader)}\")\n",
        "print(f\"Number of Classes: {len(class_names)}, Classes Names: {class_names}\")"
      ],
      "metadata": {
        "id": "1PzQTSUhJKxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Visualize a single image"
      ],
      "metadata": {
        "id": "5seCQ5QaOrc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch of images\n",
        "image_batch, label_batch = next(iter(train_dataLoader))\n",
        "\n",
        "# Get single image and label\n",
        "image, label = image_batch[8], label_batch[8]\n",
        "\n",
        "# Image shape and label\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "JNRAUAq2NA8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Image with matplotlib\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "VjwAu0I-PDDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replicating ViT: Overview\n",
        "\n",
        "* **Inputs** - Images\n",
        "* **Outputs** - Outside model/layer/block\n",
        "* **Layers** - Takes an input and manipulates it with a function (equation)\n",
        "* **Blocks** - A collection of layers\n",
        "* **Model** - A collection of blocks\n",
        "\n",
        "#### ViT Architecture\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/302ea8d0a64a52f33937084a7c0b9c5249d5f8f2/images/08-vit-paper-figure-1-architecture-overview.png' alt='ViT architecture' width='600px' />\n",
        "\n",
        "#### ViT Equiations\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png' alt='equations' width='600px' />"
      ],
      "metadata": {
        "id": "I3qJh9T-Ss06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Equations\n",
        "\n",
        "**Equiation One:**\n",
        "```python\n",
        "# Equation 1\n",
        "x_input = [class_token, image_patch_1, image_patch_2, ... image_patch_N] + [class_token_pos, image_patch_1_pos, image_patch_2_pos, ... image_patch_N_pos]\n",
        "```\n",
        "\n",
        "**Equation 2 & 3:**\n",
        "```python\n",
        "# Equation 2\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "\n",
        "# Equation 3\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "\n",
        "**Equation 4:**\n",
        "```python\n",
        "# Equation 4\n",
        "y = Linear_layer(LN_layer(x_output_MLP_block))\n",
        "```"
      ],
      "metadata": {
        "id": "rAUqZBZqdUxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Model Variants and Parameters\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-table-1.png' alt='table of the different ViT models, same architecture' width='600px' />\n",
        "\n",
        "ViT-Base, ViT-Large and ViT-Huge are different sizes of the same model architecture.\n",
        "\n",
        "* **ViT-B/16** = ViT-Base with image patch size 16x16\n",
        "* **Layers** - the number of transformer encoder layers\n",
        "* **Hidden size $D$** - the embedding size throughout the architecture\n",
        "* **MLP size** - the number of hidden units/neurons in the MLP\n",
        "* **Head** - the number of multi-head self-attention"
      ],
      "metadata": {
        "id": "q6klByfW3lYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Equation One: Split data into patches and create the class, position and patch embedding\n",
        "\n",
        "Layers = input -> function -> output\n",
        "\n",
        "* Input shape: $H\\times{W}\\times{C}$\n",
        "* Output shape: ${N \\times\\left(P^{2} \\cdot C\\right)}$\n",
        "* H = height\n",
        "* W = width\n",
        "* C = color channels\n",
        "* P = patch size\n",
        "* N = number of patches = (height * width) / P^2\n",
        "* D = contant latent vector size = embedding dimension (See on Table 1)"
      ],
      "metadata": {
        "id": "w9d1CIhO67xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example values\n",
        "height = 224\n",
        "width = 224\n",
        "color_channels = 3\n",
        "patch_size = 16\n",
        "\n",
        "# Calculate the number of patches\n",
        "number_of_patches = int((height * width) / patch_size ** 2)\n",
        "number_of_patches"
      ],
      "metadata": {
        "id": "QZFqm-y0PaPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size * patch_size * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 1D sequence of patches): {embedding_layer_output_shape}\")\n"
      ],
      "metadata": {
        "id": "C3mMqviDEK2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Turning a single image into patches"
      ],
      "metadata": {
        "id": "qqjeCQGBjlUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View a single image\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "PbPFpkS5FEa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top row of the image\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "9RBjr90Ajyy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup code to plot top row as patches\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = int(img_size/patch_size)\n",
        "assert img_size % patch_size == 0, \"Image dimensions must be divisible by the patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nNumber of patches per column: {num_patches}\\nTotal patches: {num_patches*num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=num_patches,\n",
        "                        ncols=num_patches, # One column for each patch\n",
        "                        sharex=True,\n",
        "                        sharey=True,\n",
        "                        figsize=(patch_size, patch_size))\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for j, patch_y in enumerate(range(0, img_size, patch_size)):\n",
        "  for i, patch_x in enumerate(range(0, img_size, patch_size)):\n",
        "      axs[j][i].imshow(image_permuted[patch_y:patch_y+patch_size, patch_x:patch_x+patch_size, :]);\n",
        "      axs[j, i].set_ylabel(j+1, rotation=\"horizontal\", horizontalalignment='right', verticalalignment='center')\n",
        "      axs[j][i].set_xlabel(i+1)\n",
        "      axs[j][i].set_xticks([])\n",
        "      axs[j][i].set_yticks([])\n",
        "      axs[j][i].label_outer()\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b8H-4SHPkfJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Creating image patches and turning them into patch embeddings\n",
        "\n",
        "By using `torch.nn.Conv2d()` and setting the kernel size and stride to `patch_size`."
      ],
      "metadata": {
        "id": "WekhSx4-01Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create conv2d layer to turn image into patches of learnable feature maps (embeddings)\n",
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size = 16\n",
        "\n",
        "# Create a conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, #for color images\n",
        "                   out_channels=768, # D size from table 1,\n",
        "                   kernel_size=patch_size,\n",
        "                   stride=patch_size,\n",
        "                   padding=0)\n",
        "conv2d"
      ],
      "metadata": {
        "id": "pSHonnn2miUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the image thrpught the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension -> (batch_size, color_channels, height, width)\n",
        "print(f\"{image_out_of_conv.shape} [batch_size, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "s_4mR6TK7f3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot ramdon convolutional feature maps (embeddings)\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 758), k=5)\n",
        "print(f\"Showing random convolution feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12,12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "  image_conv_feature_map = image_out_of_conv[:, idx, :, :]\n",
        "  axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy());\n",
        "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n"
      ],
      "metadata": {
        "id": "TxHrAe_R9yuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Flattening the patch embedding with `torch.nn.Flatten()`"
      ],
      "metadata": {
        "id": "OQEleOIlIgQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{image_out_of_conv.shape} [batch_size, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "UpBshqMlF9MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "flatten_layer = nn.Flatten(start_dim=2, end_dim=3)\n",
        "flatten_layer(image_out_of_conv).shape"
      ],
      "metadata": {
        "id": "qh3H8oCuS3sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put everything together\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f'Original image shape: {image.shape}')\n",
        "\n",
        "# Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0))\n",
        "print(f'Image feature map (patches) shape: {image_out_of_conv.shape}')\n",
        "\n",
        "# Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten_layer(image_out_of_conv)\n",
        "print(f'Flattened image feature map shape: {image_out_of_conv_flattened.shape}')"
      ],
      "metadata": {
        "id": "pJfPus8wTW5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rearrange output of flattened layer\n",
        "image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0, 2, 1)\n",
        "print(f\"{image_out_of_conv_flattened_permuted.shape} -> (batch_size, number_of_patches, embeding_dimension)\")"
      ],
      "metadata": {
        "id": "73K1ZEIAURIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_permuted[:, :, 0]\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22,22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy());\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "tHM5gKOpYy4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Turning the ViT patch embedding layer into a PyTorch module"
      ],
      "metadata": {
        "id": "RR7_a7MSalOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "               patch_size:int=16,\n",
        "               in_channels:int=3,\n",
        "               embedding_dim:int=768): # from Table 1 for ViT-Base\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=embedding_dim,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=patch_size,\n",
        "                             padding=0)\n",
        "    self.flatten = nn.Flatten(start_dim=2,\n",
        "                              end_dim=3)\n",
        "  def forward(self, x):\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resolution % self.patcher.stride[0] == 0, f\"Image resolution must be divisible by patch size, image shape: {image_resolution}, patch_size: {self.patch_size}\"\n",
        "    x = self.patcher(x)\n",
        "    x = self.flatten(x)\n",
        "    return x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "WbqwgnWsaAwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from modular_scripts.utils import set_seeds\n",
        "\n",
        "set_seeds(42)\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(patch_size=16, in_channels=3, embedding_dim=768)\n",
        "\n",
        "# Pass a single image through patch embedding layer\n",
        "print(f\"Input image size: {image.unsqueeze(0).shape}\")\n",
        "path_embedded_image = patchify(image.unsqueeze(0)) # Add an extra batch dimension\n",
        "print(f\"Output patch embedding sequence shape: {path_embedded_image.shape}\")\n"
      ],
      "metadata": {
        "id": "Bsd1Do5QdZF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Class token embedding"
      ],
      "metadata": {
        "id": "U236o-6kgtIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size and embedding dimension\n",
        "batch_size, embedding_dim = path_embedded_image.shape[0], path_embedded_image.shape[-1]\n",
        "batch_size, embedding_dim"
      ],
      "metadata": {
        "id": "F6YrJQBoeHLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class token embedding\n",
        "class_token = nn.Parameter(torch.randn(batch_size, 1, embedding_dim))\n",
        "class_token.shape"
      ],
      "metadata": {
        "id": "eV6MxVJrjvkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the class_token to the fron of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat([class_token, path_embedded_image], dim=1)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape:\\n{patch_embedded_image_with_class_embedding.shape} -> [batch_size, class_token + number_of_patches, embedding_dim]\")"
      ],
      "metadata": {
        "id": "PedBrzOTkJgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Creating the position embedding"
      ],
      "metadata": {
        "id": "7glR9Lx4mVCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int(height * width / patch_size ** 2)\n",
        "\n",
        "# Get the embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]\n",
        "\n",
        "# Learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.randn(1,\n",
        "                                              number_of_patches + 1,\n",
        "                                              embedding_dimension),\n",
        "                                  requires_grad=True)\n",
        "position_embedding.shape"
      ],
      "metadata": {
        "id": "zXQd1do1kkAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add position embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "patch_and_position_embedding.shape"
      ],
      "metadata": {
        "id": "n-Y1V0cUnEoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Putting it all together: from image to embedding"
      ],
      "metadata": {
        "id": "0qh2p-KpD1NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seeds\n",
        "set_seeds()\n",
        "\n",
        "path_size = 16\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "x = image.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3, embedding_dim=768, patch_size=patch_size) # patch embeding layer\n",
        "patch_embedding  = patch_embedding_layer(x) # input throught PatchEmbedding\n",
        "\n",
        "# Create class token em,bedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.randn(batch_size, 1, embedding_dimension))\n",
        "\n",
        "# Add class token to patch embedding\n",
        "patch_embedding_class_token = torch.cat([class_token, patch_embedding], dim=1)\n",
        "\n",
        "# Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.randn(1, number_of_patches + 1, embedding_dimension), requires_grad=True)\n",
        "\n",
        "# Add position embedding\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "id": "zHE9dgG-nzrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Equation 2: MSA"
      ],
      "metadata": {
        "id": "H2Di2-psWTsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  \"\"\"Creates a multi-head self-attention block (MSA)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_dim:int=768, # Hidden size D (embedding dimension) from Table 1 for ViT-Base\n",
        "               num_heads:int=12, # Heads from table 1 for ViT-Base\n",
        "               attn_dropout:int=0):\n",
        "    super().__init__()\n",
        "    # Create the norm layer(LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # Create multihead attention (MSA) layer\n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                num_heads=num_heads,\n",
        "                                                dropout=attn_dropout,\n",
        "                                                batch_first=True) # (batch, seq, feature) -> (batch, number_of_patches, embedding_dimension)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    x, _ = self.multihead_attn(query=x,\n",
        "                               key=x,\n",
        "                               value=x,\n",
        "                               need_weights=False)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ARK6_VIvLkqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance MSA block\n",
        "multihead_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dim=768,\n",
        "                                                             num_heads=12,\n",
        "                                                             attn_dropout=0)\n",
        "\n",
        "# Pass the patch and position image embedding sequence througth MSA block\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape of MSA block: {patched_image_through_msa_block.shape}\")"
      ],
      "metadata": {
        "id": "c-yAqvy_FCNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Equation 3: MLP"
      ],
      "metadata": {
        "id": "8dYtunGuP5Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim:int=768,\n",
        "               mlp_size:int=3072,\n",
        "               dropout:int=0.1):\n",
        "    super().__init__()\n",
        "    # Create the norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "    # Create the MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=mlp_size,\n",
        "                  out_features=embedding_dim),\n",
        "        nn.Dropout(p=dropout)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "JzqOWZsVK630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768,\n",
        "                     mlp_size=3072,\n",
        "                     dropout=0.1)\n",
        "\n",
        "# Pass output the mSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape of MLP block: {patched_image_through_mlp_block.shape}\")"
      ],
      "metadata": {
        "id": "e1HG0tm31aqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Transformer Encoder"
      ],
      "metadata": {
        "id": "i69yQtAX6LRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Custom transformer encoder block"
      ],
      "metadata": {
        "id": "27ezTlc9oWtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim:int=768,\n",
        "               num_heads:int=12,\n",
        "               mlp_size:int=3072,\n",
        "               mlp_dropout:int=0.1,\n",
        "               attn_dropout:int=0):\n",
        "    super().__init__()\n",
        "    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                 num_heads=num_heads,\n",
        "                                                 attn_dropout=attn_dropout)\n",
        "    self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
        "                              mlp_size=mlp_size,\n",
        "                              dropout=mlp_dropout)\n",
        "  def forward(self, x):\n",
        "    x = x + self.msa_block(x)\n",
        "    x = x + self.mlp_block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-NrMXCxJ5OZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TransformerEncoderBlock()\n",
        "transformer_encoder_block = TransformerEncoderBlock()\n",
        "\n",
        "# Get a sumary using torchinfo.summary\n",
        "summary(model=transformer_encoder_block,\n",
        "        input_size=(1, 197, 768),\n",
        "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "        col_width=20,\n",
        "        row_settings=['var_names'])"
      ],
      "metadata": {
        "id": "Sh-RFaP4F0JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Transformer Encoder Layer with in-built PyTorch layers"
      ],
      "metadata": {
        "id": "ErbcJsduoahD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
        "                                                             nhead=12,\n",
        "                                                             dim_feedfoward=3072,\n",
        "                                                             dropout=0.1,\n",
        "                                                             activation=\"gelu\",\n",
        "                                                             batch_first=True,\n",
        "                                                             norm_first=True)"
      ],
      "metadata": {
        "id": "TufhGFiGGs9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sumary using torchinfo.summary\n",
        "summary(model=torch_transformer_encoder_layer,\n",
        "        input_size=(1, 197, 768),\n",
        "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "        col_width=20,\n",
        "        row_settings=['var_names'])"
      ],
      "metadata": {
        "id": "MMXl-gwcrj_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQlUw_JurtJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}